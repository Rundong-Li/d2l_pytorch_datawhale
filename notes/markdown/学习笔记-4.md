<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>

## å­¦ä¹ ç¬”è®°-4: Task 4

## 10 - æœºå™¨ç¿»è¯‘åŠç›¸å…³æŠ€æœ¯

### æœºå™¨ç¿»è¯‘å’Œæ•°æ®é›†

#### æœºå™¨ç¿»è¯‘

å°†ä¸€æ®µæ–‡æœ¬ä»ä¸€ç§è¯­è¨€è‡ªåŠ¨ç¿»è¯‘ä¸ºå¦ä¸€ç§è¯­è¨€ï¼Œç”¨ç¥ç»ç½‘ç»œè§£å†³è¿™ä¸ªé—®é¢˜é€šå¸¸ç§°ä¸ºç¥ç»æœºå™¨ç¿»è¯‘ (NMT)  
ä¸»è¦ç‰¹å¾ï¼šè¾“å‡ºæ˜¯å•è¯åºåˆ—è€Œä¸æ˜¯å•ä¸ªå•è¯  
ä¸»è¦å›°éš¾ï¼šè¾“å‡ºåºåˆ—çš„é•¿åº¦å¯èƒ½ä¸æºåºåˆ—çš„é•¿åº¦ä¸åŒ

#### æ•°æ®é¢„å¤„ç†

å°†æ•°æ®é›†æ¸…æ´—ã€è½¬åŒ–ä¸ºç¥ç»ç½‘ç»œçš„è¾“å…¥mini-batch

#### åˆ†è¯

å­—ç¬¦ä¸² $\boldsymbol{\rightarrow}$ å•è¯ç»„æˆçš„åˆ—è¡¨

#### å»ºç«‹è¯å…¸

å•è¯ç»„æˆçš„åˆ—è¡¨ $\boldsymbol{\rightarrow}$ å•è¯idç»„æˆçš„åˆ—è¡¨

### Encoder-Decoder

Encoderï¼šè¾“å…¥åˆ°éšè—çŠ¶æ€  
Decoderï¼šéšè—çŠ¶æ€åˆ°è¾“å‡º  
é€šè¿‡Encoderï¼Œå°†è¾“å…¥è½¬åŒ–æˆä¸ºä¸€ä¸ªéšè—çŠ¶æ€ï¼Œå¹¶æŠŠå®ƒä½œä¸ºè¯­ä¹‰ç¼–ç çš„è¾“å…¥
è§£å†³è¾“å…¥è¾“å‡ºåºåˆ—é•¿åº¦ä¸åŒçš„é—®é¢˜

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5jcat3c8m.png?imageView2/0/w/640/h/640>
</div>

ä»£ç å®ç°
```python
class Encoder(nn.Module):
    def __init__(self, **kwargs):
        super(Encoder, self).__init__(**kwargs)

    def forward(self, X, *args):
    """
    é¢„ç•™forwardæ–¹æ³•æ¥å£ä¸å®ç°ï¼Œåœ¨å…¶å­ç±»ä¸­å®ç°ï¼›
    å¹¶ä¸”è¦æ±‚å…¶å­ç±»ä¸€å®šè¦å®ç°ï¼Œä¸å®ç°çš„æ—¶å€™ä¼šå¯¼è‡´é—®é¢˜ï¼›
    è¿™é‡Œäº§ç”Ÿçš„é—®é¢˜æ˜¯NotImplementedErrorã€‚
    """
        raise NotImplementedError

class Decoder(nn.Module):
    def __init__(self, **kwargs):
        super(Decoder, self).__init__(**kwargs)
    
    # è¿™é‡Œraise Errorçš„ç›®çš„å’ŒEncoderä¸­çš„ç›¸åŒ
    def init_state(self, enc_outputs, *args):
        raise NotImplementedError

    def forward(self, X, state):
        raise NotImplementedError

class EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, **kwargs):
        super(EncoderDecoder, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, enc_X, dec_X, *args):
        enc_outputs = self.encoder(enc_X, *args)
        # éœ€è¦åœ¨Decoderçš„å­ç±»ä¸­å®šä¹‰init_stateæ–¹æ³•ï¼Œæ‰èƒ½è°ƒç”¨decoderå®ä¾‹çš„init_stateæ–¹æ³•
        dec_state = self.decoder.init_state(enc_outputs, *args)
        return self.decoder(dec_X, dec_state)
```

### Sequence to Sequenceæ¨¡å‹

#### æ¨¡å‹ï¼š

è®­ç»ƒ  
<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640>
</div>

é¢„æµ‹  
<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640>
</div>

#### å…·ä½“ç»“æ„ï¼š

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500>
</div> 

#### Encoder

Sourcesæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œç»è¿‡Embeddingå±‚ä¹‹åå¾—åˆ°è¯å‘é‡
``` python
class Seq2SeqEncoder(d2l.Encoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)
   
    def begin_state(self, batch_size, device):
        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device),
                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens), device=device)]
    def forward(self, X, *args):
        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)
        # RNNçš„è¾“å…¥å¿…é¡»æ˜¯æ—¶åºçš„tensorï¼ŒXçš„ç¬¬ä¸€ä¸ªç»´åº¦åº”è¯¥æ˜¯å¥å­ä¸­å•è¯çš„é¡ºåºï¼Œå³seq_len
        X = X.transpose(0, 1)  # RNN needs first axes to be time
        # state = self.begin_state(X.shape[1], device=X.device)
        out, state = self.rnn(X)
        """
        The shape of out is (seq_len, batch_size, num_hiddens);
        state contains the hidden state and the memory cell
        of the last time step, 
        the shape of state is (num_layers, batch_size, num_hiddens).
        """
        return out, state
```
```python
encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)
X = torch.zeros((4, 7), dtype = torch.long)  # (batch_size, seq_len)
output, state = encoder(X)

output.shape
>>> torch.Size([7, 4, 16])  # (seq_len, batch_size, num_hiddens)

len(state)
>>> 2  # å› ä¸ºstateåŒ…å«hidden stateå’Œmemory cellä¸¤é¡¹

state[0].shape
>>> torch.Size([2, 4, 16])  # (num_layers, batch_size, num_hiddens)

state[1].shape
>>> torch.Size([2, 4, 16])  # (num_layers, batch_size, num_hiddens)
```

#### Decoder
```python
class Seq2SeqDecoder(d2l.Decoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqDecoder, self).__init__(**kwargs)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)
        # denseå±‚ï¼ŒæŠŠnum_hiddenså¤§å°çš„å‘é‡è½¬åŒ–ä¸ºvocab_sizeçš„å‘é‡ï¼Œä»¥ä¾¿è¾“å‡ºé¢„æµ‹çš„å•è¯
        self.dense = nn.Linear(num_hiddens,vocab_size)

    def init_state(self, enc_outputs, *args):
        # decoderåˆå§‹çŠ¶æ€è®¾ç½®ä¸ºencoderè¿”å›çš„stateï¼ŒåŒ…å«æœ€åä¸€æ­¥çš„hidden stateå’Œmemory cell
        return enc_outputs[1]  

    def forward(self, X, state):
        # è½¬ç½®ï¼Œç›®çš„åŒencoderä¸­çš„X.transpose(0, 1)
        X = self.embedding(X).transpose(0, 1)  
        out, state = self.rnn(X, state)
        # Make the batch to be the first dimension to simplify loss computation.
        out = self.dense(out).transpose(0, 1)  # åˆšåˆšè½¬ç½®è¿‡ï¼Œå†é¢ å€’å›æ¥ï¼Œç°åœ¨outæ˜¯(batch_size, seq_len, vocab_size)
        return out, state
```
```python
decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8,num_hiddens=16, num_layers=2)
state = decoder.init_state(encoder(X))  # X: (batch_size, seq_len)
out, state = decoder(X, state)

out.shape
>>> torch.Size([4, 7, 10])  # (batch_size, seq_len, vocab_size)

len(state)
>>> 2  # å› ä¸ºstateåŒ…å«hidden stateå’Œmemory cellä¸¤é¡¹

state[0].shape
>>> torch.Size([2, 4, 16])  # (num_layers, batch_size, num_hiddens)

state[1].shape
>>> torch.Size([2, 4, 16])  # (num_layers, batch_size, num_hiddens)
```

#### æŸå¤±å‡½æ•°

è®¡ç®—æŸå¤±çš„æ—¶å€™è¦å±è”½ä¹‹å‰paddingçš„éƒ¨åˆ†ï¼Œé€šè¿‡```SequenceMask```å‡½æ•°å®ç°
```python
def SequenceMask(X, X_len, value = 0):
    """
    X: å¾…å¤„ç†çš„tensor
    X_len: è¯¥tensorçš„æœ‰æ•ˆé•¿åº¦
    value: ç”¨æ¥æ›¿æ¢paddingéƒ¨åˆ†çš„å€¼, é»˜è®¤ä¸º0
    """
    maxlen = X.size(1)
    # [None, :]å’Œ[:, None]éƒ½æ˜¯ç”¨æ¥æ‰©å±•tensorç»´åº¦çš„
    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   
    X[~mask] = value
    return X
```
```python
X = torch.tensor([[1,2,3], [4,5,6]])
SequenceMask(X, torch.tensor([1,2]))
>>> tensor([[1, 0, 0],
        [4, 5, 0]])
```
```python
# æ”¹å†™nn.CrossEntropyLossçš„forwardå‡½æ•°, åŠ å…¥Maskæ“ä½œ
class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):
    # pred shape: (batch_size, seq_len, vocab_size)
    # label shape: (batch_size, seq_len)
    # valid_length shape: (batch_size, )
    def forward(self, pred, label, valid_length):
        # the sample weights shape should be (batch_size, seq_len)
        weights = torch.ones_like(label)
        weights = SequenceMask(weights, valid_length).float()
        # ç°åœ¨weightsæ˜¯ä¸€ä¸ªåªæœ‰æœ‰æ•ˆä½ç½®ä¸º1, å…¶ä½™ä½ç½®å‡ä¸º0, å½¢çŠ¶å’Œlabelä¸€è‡´çš„tensor
        self.reduction = 'none'
        output = super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)
        return (output*weights).mean(dim=1)
```

### Beam Search

ç®€å•greedy searchï¼š

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5jchqoppn.png?imageView2/0/w/440/h/440>
</div>

ç»´ç‰¹æ¯”ç®—æ³•ï¼šé€‰æ‹©æ•´ä½“åˆ†æ•°æœ€é«˜çš„å¥å­ (æœç´¢ç©ºé—´å¤ªå¤§)  

é›†æŸæœç´¢ï¼š

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640>
</div>

ä¸Šå›¾ä¸­ ```beam_size = 2```

## 11 - æ³¨æ„åŠ›æœºåˆ¶å’ŒSeq2seqæ¨¡å‹

### æ³¨æ„åŠ›æœºåˆ¶

ç¼–ç å™¨â€”è§£ç å™¨(seq2seq)ç»“æ„å­˜åœ¨ç€é—®é¢˜ï¼Œå°¤å…¶æ˜¯RNNæœºåˆ¶å®é™…ä¸­å­˜åœ¨é•¿ç¨‹æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼Œå¯¹äºè¾ƒé•¿çš„å¥å­ï¼Œæˆ‘ä»¬å¾ˆéš¾å¯„å¸Œæœ›äºå°†è¾“å…¥çš„åºåˆ—è½¬åŒ–ä¸ºå®šé•¿çš„å‘é‡è€Œä¿å­˜æ‰€æœ‰çš„æœ‰æ•ˆä¿¡æ¯ï¼Œæ‰€ä»¥éšç€æ‰€éœ€ç¿»è¯‘å¥å­çš„é•¿åº¦çš„å¢åŠ ï¼Œè¿™ç§ç»“æ„çš„æ•ˆæœä¼šæ˜¾è‘—ä¸‹é™ã€‚  

ä¸æ­¤åŒæ—¶ï¼Œè§£ç çš„ç›®æ ‡è¯è¯­å¯èƒ½åªä¸åŸè¾“å…¥çš„éƒ¨åˆ†è¯è¯­æœ‰å…³ï¼Œè€Œå¹¶ä¸æ˜¯ä¸æ‰€æœ‰çš„è¾“å…¥æœ‰å…³ã€‚ä¾‹å¦‚ï¼Œå½“æŠŠâ€œHello worldâ€ç¿»è¯‘æˆâ€œBonjour le mondeâ€æ—¶ï¼Œâ€œHelloâ€æ˜ å°„æˆâ€œBonjourâ€ï¼Œâ€œworldâ€æ˜ å°„æˆâ€œmondeâ€ã€‚åœ¨Seq2seqæ¨¡å‹ä¸­ï¼Œè§£ç å™¨åªèƒ½éšå¼åœ°ä»ç¼–ç å™¨çš„æœ€ç»ˆçŠ¶æ€ä¸­é€‰æ‹©ç›¸åº”çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å°†è¿™ç§é€‰æ‹©è¿‡ç¨‹æ˜¾å¼åœ°å»ºæ¨¡ã€‚

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5km4dwgf9.PNG?imageView2/0/w/960/h/960>
</div>

#### æ³¨æ„åŠ›æœºåˆ¶æ¡†æ¶

Attentionæ˜¯ä¸€ç§é€šç”¨çš„å¸¦æƒæ± åŒ–æ–¹æ³•ï¼Œè¾“å…¥ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼šè¯¢é—®(query)å’Œé”®å€¼å¯¹(key-value pairs)ã€‚$ğ¤_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘˜}, ğ¯_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘£}, ğªâˆˆâ„^{ğ‘‘_ğ‘}$, ```attention layer```å¾—åˆ°è¾“å‡ºä¸```value```çš„ç»´åº¦ä¸€è‡´ $ğ¨âˆˆâ„^{ğ‘‘_ğ‘£}$ã€‚å¯¹äºä¸€ä¸ª```query```æ¥è¯´ï¼Œ```attention layer```ä¼šå¯¹æ¯ä¸€ä¸ª```key```è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°å¹¶è¿›è¡Œæƒé‡çš„å½’ä¸€åŒ–ï¼Œè¾“å‡ºçš„å‘é‡$\boldsymbol{o}$åˆ™æ˜¯```value```çš„åŠ æƒæ±‚å’Œï¼Œè€Œæ¯ä¸ª```key```è®¡ç®—çš„æƒé‡ä¸```value```ä¸€ä¸€å¯¹åº”ã€‚

ä¸ºäº†è®¡ç®—è¾“å‡ºï¼Œæˆ‘ä»¬é¦–å…ˆå‡è®¾æœ‰ä¸€ä¸ªå‡½æ•°$\alpha$ç”¨äºè®¡ç®—```query```å’Œ```key```çš„ç›¸ä¼¼æ€§ï¼Œç„¶åå¯ä»¥è®¡ç®—æ‰€æœ‰çš„```attention scores``` $a_1, \ldots, a_n$ by

$$a_i = \alpha(\mathbf q, \mathbf k_i)$$

æˆ‘ä»¬ä½¿ç”¨```softmax```å‡½æ•°è·å¾—æ³¨æ„åŠ›æƒé‡ï¼š

$$b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n)$$

æœ€ç»ˆçš„è¾“å‡ºå°±æ˜¯```value```çš„åŠ æƒæ±‚å’Œï¼š

$$\mathbf o = \sum_{i=1}^n b_i \mathbf v_i$$

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960>
</div>

ä¸åŒçš„```attetion layer```é€‰æ‹©çš„```score```å‡½æ•°ä¸åŒ

#### è¶…å‡º2ç»´çŸ©é˜µçš„ä¹˜æ³•
$X$ å’Œ $Y$ æ˜¯ç»´åº¦åˆ†åˆ«ä¸º $(b,n,m)$ å’Œ $(b, m, k)$ çš„å¼ é‡ï¼Œè¿›è¡Œ $b$ æ¬¡äºŒç»´çŸ©é˜µä¹˜æ³•åå¾—åˆ° $Z$, ç»´åº¦ä¸º $(b, n, k)$

$$ Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\qquad for\quad i= 1, 2, \cdots n $$

### ç‚¹ç§¯æ³¨æ„åŠ›

```The dot product``` å‡è®¾```query```å’Œ```keys```æœ‰ç›¸åŒçš„ç»´åº¦, å³ $\forall i, ğª,ğ¤_ğ‘– âˆˆ â„_ğ‘‘$ã€‚é€šè¿‡è®¡ç®—```query```å’Œ```key```è½¬ç½®çš„ä¹˜ç§¯æ¥è®¡ç®—```attention score```ï¼Œé€šå¸¸è¿˜ä¼šé™¤å» $\sqrt{d}$ å‡å°‘è®¡ç®—å‡ºæ¥çš„scoreå¯¹ç»´åº¦ğ‘‘çš„ä¾èµ–æ€§ï¼Œå¦‚ä¸‹

$$\alpha (ğª, ğ¤) = âŸ¨ğª, ğ¤âŸ©/ \sqrt{d}$$

å‡è®¾ $ğâˆˆâ„^{ğ‘šÃ—ğ‘‘}$ æœ‰ $m$ ä¸ª```query```ï¼Œ$ğŠâˆˆâ„^{ğ‘›Ã—ğ‘‘}$ æœ‰ $n$ ä¸ª```keys```ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡çŸ©é˜µè¿ç®—çš„æ–¹å¼è®¡ç®—æ‰€æœ‰ $mn$ ä¸ª```score```ï¼š

$$ \alpha(ğ, ğŠ) = {ğğŠ}^ğ‘‡/ \sqrt{d}$$

### å¤šå±‚æ„ŸçŸ¥æœºæ³¨æ„åŠ›

åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†```query and key```æŠ•å½±åˆ° $â„^â„$ã€‚ä¸ºäº†æ›´å…·ä½“ï¼Œæˆ‘ä»¬å°†å¯ä»¥å­¦ä¹ çš„å‚æ•°åšå¦‚ä¸‹æ˜ å°„  

$$ ğ–_ğ‘˜âˆˆâ„^{â„Ã—ğ‘‘_ğ‘˜}ï¼Œğ–_ğ‘âˆˆâ„^{â„Ã—ğ‘‘_ğ‘}ï¼Œğ¯âˆˆâ„^h $$

å°†```score```å‡½æ•°å®šä¹‰ä¸º

$$ \alpha(ğ¤, ğª)=ğ¯^ğ‘‡ tanh(ğ–_ğ‘˜ ğ¤+ğ–_ğ‘ ğª) $$

ç„¶åå°†```key```å’Œ```query```åœ¨ç‰¹å¾çš„ç»´åº¦ä¸Šåˆå¹¶(concatenate)ï¼Œç„¶åé€è‡³  ```a single hidden layer perceptron```ï¼Œè¿™å±‚ä¸­```hidden layer```ä¸º $â„$ï¼Œè¾“å‡ºçš„```size```ä¸º $1$ã€‚éšå±‚æ¿€æ´»å‡½æ•°ä¸º```tanh```ï¼Œæ— åç½®ã€‚

### å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶çš„Seq2seqæ¨¡å‹

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800>
</div>

å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„```Seq2seq```çš„ç¼–ç å™¨ä¸ä¹‹å‰ç« èŠ‚ä¸­çš„```Seq2SeqEncoder```ç›¸åŒï¼›è§£ç å™¨ä¸­æ·»åŠ äº†ä¸€ä¸ª```MLP```æ³¨æ„å±‚ (MLPAttention)ï¼Œå®ƒçš„éšè—å¤§å°ä¸è§£ç å™¨ä¸­çš„```LSTM```å±‚ç›¸åŒã€‚  
ä»¥ä¸‹ä¸‰ä¸ªå‚æ•°ç”¨æ¥åˆå§‹åŒ–è§£ç å™¨çš„çŠ¶æ€:

- the encoder outputs of all timestepsï¼š```encoder```è¾“å‡ºçš„å„ä¸ªçŠ¶æ€ï¼Œè¢«ç”¨äº```attetion layer```çš„```memory```éƒ¨åˆ†ï¼Œæœ‰ç›¸åŒçš„```key```å’Œ```value```
- the hidden state of the encoderâ€™s final timestepï¼šç¼–ç å™¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œè¢«ç”¨äºåˆå§‹åŒ–```decoder```çš„```hidden state```
- the encoder valid length: ç¼–ç å™¨çš„æœ‰æ•ˆé•¿åº¦ï¼Œå€Ÿæ­¤ï¼Œæ³¨æ„å±‚ä¸ä¼šè€ƒè™‘ç¼–ç å™¨è¾“å‡ºä¸­çš„```paddings```
åœ¨è§£ç çš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œä½¿ç”¨è§£ç å™¨çš„æœ€åä¸€ä¸ª```RNN```å±‚çš„è¾“å‡ºä½œä¸ºæ³¨æ„å±‚çš„```query```ã€‚ç„¶åï¼Œå°†æ³¨æ„åŠ›æ¨¡å‹çš„è¾“å‡º```context vector```ä¸è¾“å…¥çš„åµŒå…¥å‘é‡ $D_{t}$ è¿æ¥èµ·æ¥ï¼Œè¾“å…¥åˆ°```RNN```å±‚ã€‚  

## 12 - Transformer

ä¹‹å‰ä»‹ç»çš„ä¸»æµç¥ç»ç½‘ç»œæ¶æ„CNNå’ŒRNNå…·æœ‰å¦‚ä¸‹çš„ä¼˜åŠ¿å’Œä¸è¶³ï¼š

* CNNs æ˜“äºå¹¶è¡ŒåŒ–ï¼Œå´ä¸é€‚åˆæ•æ‰å˜é•¿åºåˆ—å†…çš„ä¾èµ–å…³ç³»
* RNNs é€‚åˆæ•æ‰é•¿è·ç¦»å˜é•¿åºåˆ—çš„ä¾èµ–ï¼Œä½†æ˜¯å´éš¾ä»¥å®ç°å¹¶è¡ŒåŒ–å¤„ç†åºåˆ—

ä¸ºäº†æ•´åˆCNNå’ŒRNNçš„ä¼˜åŠ¿ï¼Œ[Vaswani et al., 2017](https://d2l.ai/chapter_references/zreferences.html#vaswani-shazeer-parmar-ea-2017) åˆ›æ–°æ€§åœ°ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è®¾è®¡äº†`Transformer`æ¨¡å‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨`attention`æœºåˆ¶å®ç°äº†å¹¶è¡ŒåŒ–æ•æ‰åºåˆ—ä¾èµ–ï¼Œå¹¶ä¸”åŒæ—¶å¤„ç†åºåˆ—çš„æ¯ä¸ªä½ç½®çš„`tokens`ï¼Œä¸Šè¿°ä¼˜åŠ¿ä½¿å¾—`Transformer`æ¨¡å‹åœ¨æ€§èƒ½ä¼˜å¼‚çš„åŒæ—¶å¤§å¤§å‡å°‘äº†è®­ç»ƒæ—¶é—´ã€‚

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5kpbj2cj5.png?imageView2/0/w/960/h/960>
</div>

ä¸Šå›¾å±•ç¤ºäº†`Transformer`æ¨¡å‹çš„æ¶æ„ï¼Œä¸`Seq2seq`æ¨¡å‹ç›¸ä¼¼ï¼Œ`Transformer`åŒæ ·åŸºäºç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œå…¶åŒºåˆ«ä¸»è¦åœ¨äºä»¥ä¸‹ä¸‰ç‚¹ï¼š

1. `Transformer blocks`ï¼šå°†`Seq2seq`æ¨¡å‹é‡çš„å¾ªç¯ç½‘ç»œæ›¿æ¢ä¸ºäº†`Transformer Blocks`ï¼Œè¯¥æ¨¡å—åŒ…å«ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼ˆMulti-head Attention Layersï¼‰ä»¥åŠä¸¤ä¸ª`position-wise feed-forward networks`(FFN)ã€‚å¯¹äºè§£ç å™¨æ¥è¯´ï¼Œå¦ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚è¢«ç”¨äºæ¥å—ç¼–ç å™¨çš„éšè—çŠ¶æ€ã€‚
2. `Add and norm`ï¼šå¤šå¤´æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç½‘ç»œçš„è¾“å‡ºè¢«é€åˆ°ä¸¤ä¸ª`add and norm`å±‚è¿›è¡Œå¤„ç†ï¼Œè¯¥å±‚åŒ…å«æ®‹å·®ç»“æ„ä»¥åŠå±‚å½’ä¸€åŒ–ã€‚
3. `Position encoding`ï¼šç”±äºè‡ªæ³¨æ„åŠ›å±‚å¹¶æ²¡æœ‰åŒºåˆ†å…ƒç´ çš„é¡ºåºï¼Œæ‰€ä»¥ä¸€ä¸ªä½ç½®ç¼–ç å±‚è¢«ç”¨äºå‘åºåˆ—å…ƒç´ é‡Œæ·»åŠ ä½ç½®ä¿¡æ¯ã€‚

### å¤šå¤´æ³¨æ„åŠ›å±‚

è‡ªæ³¨æ„åŠ›æ¨¡å‹æ˜¯ä¸€ä¸ªæ­£è§„çš„æ³¨æ„åŠ›æ¨¡å‹ï¼Œåºåˆ—çš„æ¯ä¸€ä¸ªå…ƒç´ å¯¹åº”çš„`key`ï¼Œ`value`ï¼Œ`query`æ˜¯å®Œå…¨ä¸€è‡´çš„ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè‡ªæ³¨æ„åŠ›è¾“å‡ºäº†ä¸€ä¸ªä¸è¾“å…¥é•¿åº¦ç›¸åŒçš„è¡¨å¾åºåˆ—ï¼Œä¸å¾ªç¯ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œè‡ªæ³¨æ„åŠ›å¯¹æ¯ä¸ªå…ƒç´ è¾“å‡ºçš„è®¡ç®—æ˜¯å¹¶è¡Œçš„ã€‚

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5kpckv38q.png?imageView2/0/w/320/h/320>
</div>

å¤šå¤´æ³¨æ„åŠ›å±‚åŒ…å«$h$ä¸ªå¹¶è¡Œçš„è‡ªæ³¨æ„åŠ›å±‚ï¼Œæ¯ä¸€ä¸ªè¿™ç§å±‚è¢«æˆä¸ºä¸€ä¸ª`head`ã€‚å¯¹æ¯ä¸ªå¤´æ¥è¯´ï¼Œåœ¨è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ä¹‹å‰ï¼Œå°†`query`ã€`key`å’Œ`value`ç”¨ä¸‰ä¸ªçº¿æ€§å±‚è¿›è¡Œæ˜ å°„ï¼Œè¿™$h$ä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºå°†ä¼šè¢«æ‹¼æ¥ä¹‹åè¾“å…¥æœ€åä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œæ•´åˆã€‚

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5kpcsozid.png?imageView2/0/w/640/h/640>
</div>

å‡è®¾`query`ï¼Œ`key`å’Œ`value`çš„ç»´åº¦åˆ†åˆ«æ˜¯$d_q$ã€$d_k$å’Œ$d_v$ã€‚é‚£ä¹ˆå¯¹äºæ¯ä¸€ä¸ªå¤´$i=1,\ldots,h$ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒç›¸åº”çš„æ¨¡å‹æƒé‡$W_q^{(i)} \in \mathbb{R}^{p_q\times d_q}$ã€$W_k^{(i)} \in \mathbb{R}^{p_k\times d_k}$å’Œ$W_v^{(i)} \in \mathbb{R}^{p_v\times d_v}$ï¼Œä»¥å¾—åˆ°æ¯ä¸ªå¤´çš„è¾“å‡ºï¼š

$$ o^{(i)} = attention(W_q^{(i)}q, W_k^{(i)}k, W_v^{(i)}v) $$

è¿™é‡Œçš„`attention`å¯ä»¥æ˜¯ä»»æ„çš„`attention function`ï¼Œæ¯”å¦‚å‰ä¸€èŠ‚ä»‹ç»çš„`dot-product attention`ä»¥åŠ`MLP attention`ã€‚ä¹‹åå°†æ‰€æœ‰`head`å¯¹åº”çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥ï¼Œé€å…¥æœ€åä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œæ•´åˆï¼Œè¿™ä¸ªå±‚çš„æƒé‡å¯ä»¥è¡¨ç¤ºä¸º$W_o\in \mathbb{R}^{d_0 \times hp_v}$

$$ o = W_o[o^{(1)}, \ldots, o^{(h)}] $$

æ¥ä¸‹æ¥å®ç°å¤šå¤´æ³¨æ„åŠ›ï¼Œå‡è®¾æœ‰$h$ä¸ªå¤´ï¼Œéšè—å±‚ç»´åº¦`hidden size`$ = p_q = p_k = p_v$ä¸`query`ï¼Œ`key`ï¼Œ`value`ç»è¿‡`Dense`å±‚ä¹‹åè¾“å‡ºçš„ç»´åº¦ä¸€è‡´ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå› ä¸ºå¤šå¤´æ³¨æ„åŠ›å±‚ä¿æŒè¾“å…¥ä¸è¾“å‡ºå¼ é‡çš„ç»´åº¦ä¸å˜ï¼Œæ‰€ä»¥è¾“å‡º`feature`çš„ç»´åº¦ä¹Ÿè®¾ç½®ä¸º$d_0 =$`hidden size`ã€‚

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_heads, dropout, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = DotProductAttention(dropout)
        self.W_q = nn.Linear(input_size, hidden_size, bias=False)
        self.W_k = nn.Linear(input_size, hidden_size, bias=False)
        self.W_v = nn.Linear(input_size, hidden_size, bias=False)
        self.W_o = nn.Linear(hidden_size, hidden_size, bias=False)
    
    def forward(self, query, key, value, valid_length):
        # query, key, and value shape: (batch_size, seq_len, dim),
        # where seq_len is the length of input sequence
        # valid_length shape is either (batch_size, )
        # or (batch_size, seq_len).

        # Project and transpose query, key, and value from
        # (batch_size, seq_len, hidden_size * num_heads) to
        # (batch_size * num_heads, seq_len, hidden_size).
        
        query = transpose_qkv(self.W_q(query), self.num_heads)
        key = transpose_qkv(self.W_k(key), self.num_heads)
        value = transpose_qkv(self.W_v(value), self.num_heads)
        
        if valid_length is not None:
            # Copy valid_length by num_heads times
            device = valid_length.device
            valid_length = valid_length.cpu().numpy() if valid_length.is_cuda else valid_length.numpy()
            if valid_length.ndim == 1:
                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))
            else:
                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,1)))

            valid_length = valid_length.to(device)
            
        output = self.attention(query, key, value, valid_length)
        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)

def transpose_qkv(X, num_heads):
    # Original X shape: (batch_size, seq_len, hidden_size * num_heads),
    # -1 means inferring its value, after first reshape, X shape:
    # (batch_size, seq_len, num_heads, hidden_size)
    X = X.view(X.shape[0], X.shape[1], num_heads, -1)
    
    # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)
    X = X.transpose(2, 1).contiguous()

    # Merge the first two dimensions. Use reverse=True to infer shape from
    # right to left.
    # output shape: (batch_size * num_heads, seq_len, hidden_size)
    output = X.view(-1, X.shape[2], X.shape[3])
    return output

# è¿˜åŸæˆç±»ä¼¼åˆå§‹çš„ç»´åº¦
def transpose_output(X, num_heads):
    # A reversed version of transpose_qkv
    X = X.view(-1, num_heads, X.shape[1], X.shape[2])
    X = X.transpose(2, 1).contiguous()
    return X.view(X.shape[0], X.shape[1], -1)
```

### åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ

`Transformer`æ¨¡å—å¦ä¸€ä¸ªéå¸¸é‡è¦çš„éƒ¨åˆ†å°±æ˜¯åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ(FFN)ï¼Œå®ƒæ¥å—ä¸€ä¸ªå½¢çŠ¶ä¸º`(batch_sizeï¼Œseq_length, feature_size)`çš„ä¸‰ç»´å¼ é‡ã€‚`Position-wise FFN`ç”±ä¸¤ä¸ªå…¨è¿æ¥å±‚ç»„æˆï¼Œå®ƒä»¬ä½œç”¨åœ¨æœ€åä¸€ç»´ä¸Šã€‚å› ä¸ºåºåˆ—çš„æ¯ä¸ªä½ç½®çš„çŠ¶æ€éƒ½ä¼šè¢«å•ç‹¬åœ°æ›´æ–°ï¼Œæ‰€ä»¥æˆ‘ä»¬ç§°å®ƒä¸º`position-wise`ï¼Œè¿™ç­‰æ•ˆäºä¸€ä¸ª1x1çš„å·ç§¯ã€‚  

```python
class PositionWiseFFN(nn.Module):
    def __init__(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)
        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)
    
    def forward(self, X):
        return self.ffn_2(F.relu(self.ffn_1(X)))
```

ä¸å¤šå¤´æ³¨æ„åŠ›å±‚ç›¸ä¼¼ï¼Œ`FFN`å±‚åŒæ ·åªä¼šå¯¹æœ€åä¸€ç»´çš„å¤§å°è¿›è¡Œæ”¹å˜ï¼›é™¤æ­¤ä¹‹å¤–ï¼Œå¯¹äºä¸¤ä¸ªå®Œå…¨ç›¸åŒçš„è¾“å…¥ï¼Œ`FFN`å±‚çš„è¾“å‡ºä¹Ÿå°†ç›¸ç­‰ã€‚

### Add and Norm

é™¤äº†ä¸Šé¢ä¸¤ä¸ªæ¨¡å—ä¹‹å¤–ï¼Œ`Transformer`è¿˜æœ‰ä¸€ä¸ªé‡è¦çš„ç›¸åŠ å½’ä¸€åŒ–å±‚ï¼Œå®ƒå¯ä»¥å¹³æ»‘åœ°æ•´åˆè¾“å…¥å’Œå…¶ä»–å±‚çš„è¾“å‡ºï¼Œå› æ­¤åœ¨æ¯ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚å’Œ`FFN`å±‚åé¢éƒ½æ·»åŠ ä¸€ä¸ªå«æ®‹å·®è¿æ¥çš„`Layer Norm`å±‚ã€‚è¿™é‡Œ`Layer Norm`ä¸`Batch Norm`å¾ˆç›¸ä¼¼ï¼Œå”¯ä¸€çš„åŒºåˆ«åœ¨äº`Batch Norm`æ˜¯å¯¹äº`batch size`è¿™ä¸ªç»´åº¦è¿›è¡Œè®¡ç®—å‡å€¼å’Œæ–¹å·®çš„ï¼Œè€Œ`Layer Norm`åˆ™æ˜¯å¯¹æœ€åä¸€ç»´è¿›è¡Œè®¡ç®—ã€‚å±‚å½’ä¸€åŒ–å¯ä»¥é˜²æ­¢å±‚å†…çš„æ•°å€¼å˜åŒ–è¿‡å¤§ï¼Œä»è€Œæœ‰åˆ©äºåŠ å¿«è®­ç»ƒé€Ÿåº¦å¹¶ä¸”æé«˜æ³›åŒ–æ€§èƒ½ [(ref)](https://zhuanlan.zhihu.com/p/54530247)ã€‚

```python
class AddNorm(nn.Module):
    def __init__(self, hidden_size, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(hidden_size)
    def forward(self, X, Y):
        return self.norm(self.dropout(Y) + X)
```

### ä½ç½®ç¼–ç 

ä¸å¾ªç¯ç¥ç»ç½‘ç»œä¸åŒï¼Œæ— è®ºæ˜¯å¤šå¤´æ³¨æ„åŠ›ç½‘ç»œè¿˜æ˜¯å‰é¦ˆç¥ç»ç½‘ç»œéƒ½æ˜¯ç‹¬ç«‹åœ°å¯¹æ¯ä¸ªä½ç½®çš„å…ƒç´ è¿›è¡Œæ›´æ–°ï¼Œè¿™ç§ç‰¹æ€§å¸®åŠ©æˆ‘ä»¬å®ç°äº†é«˜æ•ˆçš„å¹¶è¡Œï¼Œå´ä¸¢å¤±äº†é‡è¦çš„åºåˆ—é¡ºåºçš„ä¿¡æ¯ã€‚ä¸ºäº†æ›´å¥½çš„æ•æ‰åºåˆ—ä¿¡æ¯ï¼Œ`Transformer`æ¨¡å‹å¼•å…¥äº†ä½ç½®ç¼–ç å»ä¿æŒè¾“å…¥åºåˆ—å…ƒç´ çš„ä½ç½®ã€‚  
å‡è®¾è¾“å…¥åºåˆ—çš„åµŒå…¥è¡¨ç¤º $X\in \mathbb{R}^{l\times d}$, åºåˆ—é•¿åº¦ä¸º$l$åµŒå…¥å‘é‡ç»´åº¦ä¸º$d$ï¼Œåˆ™å…¶ä½ç½®ç¼–ç ä¸º$P \in \mathbb{R}^{l\times d}$ ï¼Œè¾“å‡ºçš„å‘é‡å°±æ˜¯äºŒè€…ç›¸åŠ  $X + P$ã€‚  
ä½ç½®ç¼–ç æ˜¯ä¸€ä¸ªäºŒç»´çš„çŸ©é˜µï¼Œ`i`å¯¹åº”ç€åºåˆ—ä¸­çš„é¡ºåºï¼Œ`j`å¯¹åº”å…¶`embedding vector`å†…éƒ¨çš„ç»´åº¦ç´¢å¼•ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹ç­‰å¼è®¡ç®—ä½ç½®ç¼–ç ï¼š

$$ P_{i,2j} = sin(i/10000^{2j/d}) $$

$$ P_{i,2j+1} = cos(i/10000^{2j/d}) $$

$$ for\ i=0,\ldots, l-1\qquad j=0,\ldots,\lfloor (d-1)/2 \rfloor $$

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5kpe0lu38.png?imageView2/0/w/640/h/640>
</div>

```python
class PositionalEncoding(nn.Module):
    def __init__(self, embedding_size, dropout, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(dropout)
        self.P = np.zeros((1, max_len, embedding_size))
        X = np.arange(0, max_len).reshape(-1, 1) / np.power(
            10000, np.arange(0, embedding_size, 2)/embedding_size)
        self.P[:, :, 0::2] = np.sin(X)
        self.P[:, :, 1::2] = np.cos(X)
        self.P = torch.FloatTensor(self.P)
    
    def forward(self, X):
        if X.is_cuda and not self.P.is_cuda:
            self.P = self.P.cuda()
        X = X + self.P[:, :X.shape[1], :]
        return self.dropout(X)
```

### ç¼–ç å™¨

ç°åœ¨å·²ç»æœ‰äº†ç»„æˆ`Transformer`çš„å„ä¸ªæ¨¡å—ï¼Œå¯ä»¥å¼€å§‹æ­å»ºäº†ï¼ç¼–ç å™¨åŒ…å«ä¸€ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œä¸€ä¸ª`position-wise FFN`ï¼Œå’Œä¸¤ä¸ª`Add and Norm`å±‚ã€‚å¯¹äº`attention`æ¨¡å‹ä»¥åŠ`FFN`æ¨¡å‹ï¼Œè¾“å‡ºç»´åº¦éƒ½æ˜¯ä¸`embedding`ç»´åº¦ä¸€è‡´çš„ï¼Œè¿™æ˜¯ç”±æ®‹å·®è¿æ¥å¤©ç”Ÿçš„ç‰¹æ€§å¯¼è‡´çš„ï¼Œå› ä¸ºè¦å°†å‰ä¸€å±‚çš„è¾“å‡ºä¸åŸå§‹è¾“å…¥ç›¸åŠ å¹¶å½’ä¸€åŒ–ã€‚  

```python
class EncoderBlock(nn.Module):
    def __init__(self, embedding_size, ffn_hidden_size, num_heads,
                 dropout, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
        self.addnorm_1 = AddNorm(embedding_size, dropout)
        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)
        self.addnorm_2 = AddNorm(embedding_size, dropout)

    def forward(self, X, valid_length):
        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))
        return self.addnorm_2(Y, self.ffn(Y))
```

ç°åœ¨æ¥å®ç°æ•´ä¸ª`Transformer`ç¼–ç å™¨æ¨¡å‹ï¼Œæ•´ä¸ªç¼–ç å™¨ç”± $n$ ä¸ªåˆšåˆšå®šä¹‰çš„`Encoder Block`å †å è€Œæˆï¼Œå› ä¸ºæ®‹å·®è¿æ¥çš„ç¼˜æ•…ï¼Œä¸­é—´çŠ¶æ€çš„ç»´åº¦å§‹ç»ˆä¸åµŒå…¥å‘é‡çš„ç»´åº¦ $d$ ä¸€è‡´ï¼›åŒæ—¶æˆ‘ä»¬æŠŠåµŒå…¥å‘é‡ä¹˜ä»¥ $\sqrt{d}$ ä»¥é˜²æ­¢å…¶å€¼è¿‡å°ã€‚  

```python
class TransformerEncoder(d2l.Encoder):
    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.embedding_size = embedding_size
        self.embed = nn.Embedding(vocab_size, embedding_size)
        self.pos_encoding = PositionalEncoding(embedding_size, dropout)
        self.blks = nn.ModuleList()
        for i in range(num_layers):
            self.blks.append(
                EncoderBlock(embedding_size, ffn_hidden_size,
                             num_heads, dropout))

    def forward(self, X, valid_length, *args):
        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))
        for blk in self.blks:
            X = blk(X, valid_length)
        return X
```

### è§£ç å™¨

`Transformer`æ¨¡å‹çš„è§£ç å™¨ä¸ç¼–ç å™¨ç»“æ„ç±»ä¼¼ï¼Œç„¶è€Œï¼Œé™¤äº†ä¸Šè¿°å‡ ä¸ªæ¨¡å—ä¹‹å¤–ï¼Œç¼–ç å™¨éƒ¨åˆ†æœ‰å¦ä¸€ä¸ªå­æ¨¡å—ã€‚è¯¥æ¨¡å—ä¹Ÿæ˜¯å¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œæ¥å—ç¼–ç å™¨çš„è¾“å‡ºä½œä¸º`key`å’Œ`value`ï¼Œ`decoder`çš„çŠ¶æ€ä½œä¸º`query`ã€‚ä¸ç¼–ç å™¨éƒ¨åˆ†ç›¸ç±»ä¼¼ï¼Œè§£ç å™¨åŒæ ·æ˜¯ä½¿ç”¨äº†`add and norm`æœºåˆ¶ï¼Œç”¨æ®‹å·®å’Œå±‚å½’ä¸€åŒ–å°†å„ä¸ªå­å±‚çš„è¾“å‡ºç›¸è¿ã€‚

ä»”ç»†æ¥è®²ï¼Œåœ¨ç¬¬$t$ä¸ªæ—¶é—´æ­¥ï¼Œå½“å‰è¾“å…¥$x_t$æ˜¯`query`ï¼Œé‚£ä¹ˆ`self attention`æ¥å—äº†ç¬¬$t$æ­¥ä»¥åŠå‰$t-1$æ­¥çš„æ‰€æœ‰è¾“å…¥$x_1,\ldots, x_{t-1}$ã€‚åœ¨è®­ç»ƒæ—¶ï¼Œç”±äºç¬¬`t`ä½ç½®çš„è¾“å…¥å¯ä»¥è§‚æµ‹åˆ°å…¨éƒ¨çš„åºåˆ—ï¼Œè¿™ä¸é¢„æµ‹é˜¶æ®µçš„æƒ…å½¢é¡¹çŸ›ç›¾ï¼Œæ‰€ä»¥è¦å°†ç¬¬`t`ä¸ªæ—¶é—´æ­¥æ‰€å¯¹åº”çš„å¯è§‚æµ‹é•¿åº¦è®¾ç½®ä¸º`t`ï¼Œä»¥æ¶ˆé™¤ä¸éœ€è¦çœ‹åˆ°çš„æœªæ¥çš„ä¿¡æ¯ã€‚

<div align = center>
    <img src = https://cdn.kesci.com/upload/image/q5kpefhcyg.png?imageView2/0/w/800/h/800>
</div>

```python
# ä¸€ä¸ªDecoderBlockçš„å®ç°
class DecoderBlock(nn.Module):
    def __init__(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.i = i
        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
        self.addnorm_1 = AddNorm(embedding_size, dropout)
        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)
        self.addnorm_2 = AddNorm(embedding_size, dropout)
        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)
        self.addnorm_3 = AddNorm(embedding_size, dropout)
    
    def forward(self, X, state):
        enc_outputs, enc_valid_length = state[0], state[1]
        
        # state[2][self.i] stores all the previous t-1 query state of layer-i
        # len(state[2]) = num_layers
        
        # If training:
        #     state[2] is useless.
        # If predicting:
        #     In the t-th timestep:
        #         state[2][self.i].shape = (batch_size, t-1, hidden_size)
        # Demo:
        # love dogs ! [EOS]
        #  |    |   |   |
        #   Transformer 
        #    Decoder
        #  |   |   |   |
        #  I love dogs !
        
        if state[2][self.i] is None:
            key_values = X
        else:
            # shape of key_values = (batch_size, t, hidden_size)
            key_values = torch.cat((state[2][self.i], X), dim=1) 
        state[2][self.i] = key_values
        
        if self.training:
            batch_size, seq_len, _ = X.shape
            # Shape: (batch_size, seq_len), the values in the j-th column are j+1
            valid_length = torch.FloatTensor(np.tile(np.arange(1, seq_len+1), (batch_size, 1))) 
            valid_length = valid_length.to(X.device)
        else:
            valid_length = None

        X2 = self.attention_1(X, key_values, key_values, valid_length)
        Y = self.addnorm_1(X, X2)
        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)
        Z = self.addnorm_2(Y, Y2)
        return self.addnorm_3(Z, self.ffn(Z)), state
```

å¯¹äº`Transformer`è§£ç å™¨æ¥è¯´ï¼Œæ„é€ æ–¹å¼ä¸ç¼–ç å™¨ä¸€æ ·ï¼Œé™¤äº†æœ€åä¸€å±‚æ·»åŠ ä¸€ä¸ª`dense layer`ä»¥è·å¾—è¾“å‡ºçš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚ä¸‹é¢å®ç°`Transformer Decoder`ï¼Œé™¤äº†å¸¸è§„çš„è¶…å‚æ•°ä¾‹å¦‚`vocab_size`ï¼Œ`embedding_size`ä¹‹å¤–ï¼Œè§£ç å™¨è¿˜éœ€è¦ç¼–ç å™¨çš„è¾“å‡º`enc_outputs`å’Œå¥å­æœ‰æ•ˆé•¿åº¦`enc_valid_length`ã€‚

```python
class TransformerDecoder(d2l.Decoder):
    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.embedding_size = embedding_size
        self.num_layers = num_layers
        self.embed = nn.Embedding(vocab_size, embedding_size)
        self.pos_encoding = PositionalEncoding(embedding_size, dropout)
        self.blks = nn.ModuleList()
        for i in range(num_layers):
            self.blks.append(
                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,
                             dropout, i))
        self.dense = nn.Linear(embedding_size, vocab_size)

    def init_state(self, enc_outputs, enc_valid_length, *args):
        return [enc_outputs, enc_valid_length, [None]*self.num_layers]

    def forward(self, X, state):
        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))
        for blk in self.blks:
            X, state = blk(X, state)
        return self.dense(X), state
```